{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística\n",
    "\n",
    "Actualmente uno de los algoritmos más ampliamente usados en el problema de clasificación es el de regresión\n",
    "logı́stica. Algunos ejemplos del problema de clasificación son:\n",
    "\n",
    "email ¿spam / no spam ?\n",
    "\n",
    "llamada ¿spam / no spam ?\n",
    "\n",
    "transacción en lı́nea ¿fraudulenta (sı́ / no)?\n",
    "\n",
    "tumor ¿maligno / benigno?\n",
    "\n",
    "En todos los problemas de clasificación simple tenemos una variable  $y \\in \\{0,1\\}$,\n",
    "\n",
    "donde: $y=\\begin{cases}\n",
    "0:& \\text{clase negativa, ejemplo tumor benigno},\\\\\n",
    "1:& \\text{clase positiva, ejemplo tumor maligno}.\n",
    "\\end{cases}$\n",
    "\n",
    "También está el problema de clasificación multiclase, donde tenemos varios posibles casos de clasificación, ejemplo clasificar el canto de un pájaro:\n",
    "$y \\in \\{0,1,2,3,4,5\\}$,\n",
    "\n",
    "donde: $y=\\begin{cases}\n",
    "0:& \\text{clase negativa, ejemplo no es ningún canto},\\\\\n",
    "1:& \\text{clase  1,  canto gorrión},\\\\\n",
    "2:& \\text{clase  2,  canto golondrina},\\\\\n",
    "3:& \\text{clase  3,  canto jilguero},\\\\\n",
    "4:& \\text{clase  4,  canto canario},\\\\\n",
    "5:& \\text{clase  5,  canto chachalaca},\n",
    "\\end{cases}$\n",
    "\n",
    "otro ejemplo, tipo de email:\n",
    "$y \\in \\{1,2,3,4\\}$,\n",
    "\n",
    "donde: $y=\\begin{cases}\n",
    "1:& \\text{clase  1, trabajo},\\\\\n",
    "2:& \\text{clase  2, amigos},\\\\\n",
    "3:& \\text{clase  3, familia},\\\\\n",
    "4:& \\text{clase  4, hobby}.\n",
    "\\end{cases}$\n",
    "\n",
    "otro ejemplo, pronosticar el tiempo:\n",
    "$y \\in \\{1,2,3,4\\}$,\n",
    "\n",
    "donde: $y=\\begin{cases}\n",
    "1:& \\text{clase  1, soleado},\\\\\n",
    "2:& \\text{clase  2, nublado},\\\\\n",
    "3:& \\text{clase  3, lluvia},\\\\\n",
    "4:& \\text{clase  4, nieve}.\n",
    "\\end{cases}$\n",
    "\n",
    "Sin embargo, como veremos más adelante el problema de clasificación múltiple puede construirse a partir del problema de clasificación simple, el cual es: y = 0 ó 1\n",
    "\n",
    "## Clasificador\n",
    "Entrenamientos (tamano del tumor - maligno), \n",
    "Tenemos los siguientes entrenamientos:\n",
    "\n",
    " 1.00000000e+00 0.00000000e+00\n",
    " \n",
    " 2.00000000e+00 0.00000000e+00\n",
    " \n",
    " 3.00000000e+00 0.00000000e+00\n",
    " \n",
    " 4.00000000e+00 0.00000000e+00\n",
    " \n",
    " 6.00000000e+00 1.00000000e+00\n",
    " \n",
    " 7.00000000e+00 1.00000000e+00\n",
    " \n",
    " 8.00000000e+00 1.00000000e+00\n",
    " \n",
    " 9.00000000e+00 1.00000000e+00\n",
    " \n",
    " \n",
    " Que nos da la siguiente gráfica:\n",
    "\n",
    " \n",
    " <img src=\"imagenes/tumor.png\" width=\"600\" height=\"300\">\n",
    " \n",
    " Buscando nuestra $\\hat{y}$ y graficando, quedaría así:\n",
    " \n",
    " <img src=\"imagenes/tumor2.png\" width=\"600\" height=\"300\">\n",
    " \n",
    " Como acabamos de ver, la regresión lineal como tal, no nos sirve para decidir si el tamaño del tumor es maligno o no.\n",
    "\n",
    "Debido principalmente a que el problema de clasificación esta acotada a: $y= \\{0 \\text{ ó } 1\\}$\n",
    "\n",
    "mientras que la $\\hat{y}$ es una ecuación que no esta acotada.\n",
    "\n",
    "## Regresión Logística\n",
    "\n",
    "Lo que hacemos entonces es usar un algoritmo de clasificación conocido como regresión logística, donde: $0 \\leq \\hat{y} \\leq 1$ \n",
    "\n",
    "básicamente lo que se hace es convertir nuestra $\\hat{y}= b + \\omega X$ del modelo de regresión lineal, al nuevo modelo de regresión logística: $\\hat{y} = \\sigma(b + \\omega X)$, donde $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Donde ahora $\\hat{y} = P(Y=1 | X)$\n",
    "\n",
    "por lo que quedaría nuestra nueva hipótesis de regresión logística de la siguiente manera:  $\\hat{y}=\\frac{1}{1+e^{- (b + \\omega X)}}$, de ésta manera acotamos la hipótesis a los valores intermedios entre 0 y 1; ya que la gráfica de ésta ecuación es asintótica a 0 y 1; como se ve en la siguiente gráfica.\n",
    "\n",
    "<img src=\"imagenes/regresionLogistica.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "\n",
    " La interpretación de lo anterior es que nuestra hipótesis $\\hat{y}$ nos da la probabilidad estimada de que $Y=1$ en una entrada $X$\n",
    "\n",
    "Ejemplo: si $x=\\biggl[\\begin{matrix}b\\\\X_1\\end{matrix}\\biggr]=\n",
    "\\biggl[\\begin{matrix}1\\\\\\text{tamaño del tumor}\\end{matrix}\\biggr]\n",
    "$\n",
    "\n",
    "si $\\hat{y}=0.7$\n",
    "\n",
    "Indicaría al paciente que tiene un 70\\% de que el tumor sea maligno.\n",
    "\n",
    "Sabemos que:\n",
    "\\begin{align}\n",
    " \\hat{y}=\\sigma(b+ \\omega^TX) \\\\\n",
    " \\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "\\end{align}\n",
    "\n",
    "Supón que queremos predecir que $\\hat{y}=1$, como vemos en la imagen de $\\sigma(z)$, ésto sucede cuando $\\hat{y} \\geq 0.5$; lo que a su vez sucede cuando $b + \\omega^Tx \\geq 0$\n",
    "\n",
    "Para predecir que $\\hat{y}=0$, como vemos ésto sucede cuando $\\hat{y} < 0.5$; lo que a su vez sucede cuando $b +\\omega^Tx < 0$\n",
    "\n",
    "## Ejemplo 1\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "datos = pd.read_csv(\"./data/fronteralinealdecision.csv\")\n",
    "datos = datos.to_numpy()\n",
    "#print(f\"datos= \\n{datos}\")\n",
    "m,n = datos.shape\n",
    "X = datos[:,0:n-1]\n",
    "Y = datos[:,n-1:n]\n",
    "X0 = [j for j in range(m) if Y[j]==0]\n",
    "X1 = [j for j in range(m) if Y[j]==1]\n",
    "print(f\"X0= \\n{X0}\")\n",
    "print(f\"X1= \\n{X1}\")\n",
    "\n",
    "plt.plot(X[X0,0],X[X0,1],\"bo\")\n",
    "plt.plot(X[X1,0],X[X1,1],\"ro\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si suponemos que $\\hat{y}= \\sigma(b + \\omega_1 x_1 + \\omega_2 x_2)$\n",
    "\n",
    "Por lo pronto supondremos  también los valores de los parámetros con los siguientes valores:\n",
    "\n",
    "\\begin{align}\n",
    "b = -3 \\\\\n",
    "\\omega_1 = 1 \\\\\n",
    "\\omega_2 = 1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "entonces tenemos: $\\hat{y} = \\sigma(-3+ x_1 + x_2)$\n",
    "\n",
    "Si queremos predecir cuando $\\hat{y}=1$, entonces como habíamos mencionado ésto sucede cuando $\\hat{y} \\geq 0.5$; lo que a su vez sucede cuando $b + \\omega^TX \\geq 0$, y sí $b + \\omega^TX = -3+ x_1 + x_2$, por lo que $\\hat{y} =1$, cuando: $-3+ x_1 + x_2 \\geq 0$; es decir cuando $x_1 + x_2 \\geq 3$; que nos da la siguiente frontera lineal de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimo = np.min(X[:,0])\n",
    "maximo = np.max(X[:,0])\n",
    "#print(minimo)\n",
    "#print(maximo)\n",
    "X2 = np.linspace(minimo, maximo, 100)\n",
    "X3 = [3-X2[j] for j in range(100)]\n",
    "#print(X2)\n",
    "#print(X3)\n",
    "plt.plot(X[X0,0],X[X0,1],\"bo\")\n",
    "plt.plot(X[X1,0],X[X1,1],\"ro\")\n",
    "plt.plot(X2,X3,\"g-\")\n",
    "plt.xlabel(\"$X1$\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Programa para crear el dato de prueba\n",
    "ancho_X = 20\n",
    "alto_Y = 20\n",
    "tope = 3\n",
    "X = np.ones([ancho_X * alto_Y, 3])\n",
    "for i in range(ancho_X):\n",
    "    for j in range(alto_Y):\n",
    "        X[alto_Y*i+j,0] = i*tope/ancho_X #X1\n",
    "        X[alto_Y*i+j,1] = j*tope/ancho_X #X2\n",
    "        X[alto_Y*i+j,2] = 0.5    #Y\n",
    "df = pd.DataFrame(X)\n",
    "df.to_csv(\"./data/datosPrueba.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoide(t):\n",
    "    return 1/(1+np.exp(-t))\n",
    "  \n",
    "\n",
    "def getPrediccionRL(b, W, X):\n",
    "    Yp = sigmoide((np.dot(X, W)+b))\n",
    "    return Yp\n",
    "\n",
    "datos = pd.read_csv(\"./data/datosPrueba.csv\")\n",
    "#datosPrueba\n",
    "#fronteralinealdecision\n",
    "datos = datos.to_numpy()\n",
    "#print(f\"datos= \\n{datos}\")\n",
    "m,n = datos.shape\n",
    "X = datos[:,0:n-1]\n",
    "Y = datos[:,n-1:n]\n",
    "\n",
    "plt.plot(X[:,0],X[:,1],\"bo\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.grid()\n",
    "plt.title(\"Datos de prueba\")\n",
    "plt.show()\n",
    "\n",
    "b = -3\n",
    "W = np.array([[n-1],[1]])\n",
    "W[0,0] = 1\n",
    "W[1,0] = 1\n",
    "\n",
    "Yp = getPrediccionRL(b, W, X)\n",
    "\n",
    "X0 = [j for j in range(m) if Yp[j]<0.5]\n",
    "X1 = [j for j in range(m) if Yp[j]>=0.5]\n",
    "\n",
    "\n",
    "plt.plot(X[X0,0],X[X0,1],\"bo\")\n",
    "plt.plot(X[X1,0],X[X1,1],\"ro\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.grid()\n",
    "plt.title(\"Regresión Logística\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frontera No-Lineal de Decisión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "datos = pd.read_csv(\"./data/fronteranolinealdecision.csv\")\n",
    "datos = datos.to_numpy()\n",
    "print(f\"datos= \\n{datos}\")\n",
    "m,n = datos.shape\n",
    "X = datos[:,0:n-1]\n",
    "Y = datos[:,n-1:n]\n",
    "X0 = [j for j in range(m) if Y[j]==0]\n",
    "X1 = [j for j in range(m) if Y[j]==1]\n",
    "#print(f\"X0= \\n{X[X0,0]}\")\n",
    "#print(f\"X1= \\n{X1}\")\n",
    "\n",
    "plt.plot(X[X0,0],X[X0,1],\"bo\")\n",
    "plt.plot(X[X1,0],X[X1,1],\"ro\")\n",
    "plt.xlabel(\"$X1$\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si suponemos que $\\hat{y}= \\sigma(b + \\omega_1 X_1 + \\omega_2 X_2 + \\omega_3 X_1^2 + \\omega_4 X_2^2)$\n",
    "\n",
    "Por lo pronto supondremos  también los valores de $\\omega_i$, con los siguientes valores:\n",
    "\\begin{align}\n",
    " b = -1 \\\\\n",
    " \\omega_1= 0 \\\\\n",
    " \\omega_2= 0 \\\\\n",
    " \\omega_3= 1 \\\\\n",
    " \\omega_4= 1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "entonces tenemos: $\\hat{y}= \\sigma(-1+ X_1^2 + X_2^2)$\n",
    "\n",
    "Si queremos predecir cuando $\\hat{y}=1$, entonces como habíamos mencionado ésto sucede cuando $\\hat{y} \\geq 0.5$; lo que a su vez sucede cuando $b +\\omega^TX \\geq 0$, y sí $b + \\omega^TX = -1+ X_1^2 + X_2^2$}, por lo que $\\hat{y}=1$, cuando: $-1+ X_1^2 + X_2^2 \\geq 0$; es decir cuando $b + X_1^2 + X_2^2 \\geq 1$; que nos da la siguiente frontera lineal de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimo = np.min(X[X0,0])\n",
    "maximo = np.max(X[X0,0])\n",
    "print(minimo)\n",
    "print(maximo)\n",
    "X2 = np.linspace(minimo, maximo, 100)\n",
    "X3 = [np.sqrt(1-X2[j]**2) for j in range(100)]\n",
    "X4 = [-np.sqrt(1-X2[j]**2) for j in range(100)]\n",
    "#print(X2)\n",
    "#print(X3)\n",
    "plt.plot(X[X0,0],X[X0,1],\"bo\")\n",
    "plt.plot(X[X1,0],X[X1,1],\"ro\")\n",
    "plt.plot(X2,X3,\"g-\")\n",
    "plt.plot(X2,X4,\"g-\")\n",
    "plt.xlabel(\"$X1$\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de Costo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este momento hemos visto los ejemplos con los valores de los parámetros resueltos, sin embargo, ¿cómo escogemos los valores correctos de los parámetros?\n",
    "\n",
    "Repasando como calculábamos la función de costo en regresión lineal:\n",
    "\n",
    "\n",
    "$J=\\frac{1}{m}{‎‎\\sum}_{i=1}^m \\mathscr{L}^i $\n",
    "\n",
    "y donde:\n",
    "\n",
    "$\\mathscr{L} = \\frac{1}{2}(\\hat{y}-y)^2 $\n",
    "\n",
    "$\\hat{y}=\\sigma(b+ \\omega^TX)$\n",
    "\n",
    "$\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Necesitamos un algoritmo nuevamente para buscar los valores de $\\omega$; y el algoritmo que veremos es similar al de descenso del gradiente que vimos en regresión lineal; con la diferencia de que el error calculado ya no es la diferencia entre la hipótesis y la salida del entrenamiento $Y$; dado que ahora $Y$ tiene solamente 2 valores posibles.\n",
    "\n",
    "Ahora el error lo calcularemos de la siguiente manera:\n",
    "\n",
    "Donde: $\\mathscr{L}= \\begin{cases}\n",
    "si\\;y=1:& -log(\\hat{y}) \\\\\n",
    "si\\;y=0:& -log(1-\\hat{y}) \\\\\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "Donde la idea es que la penalización que dará el algoritmo a las $\\hat{y}$ equivocadas será alta.\n",
    "\n",
    " <img src=\"imagenes/regresionLogistica_costo.png\" width=\"400\" height=\"300\"> \n",
    " \n",
    " Resumiendo lo anterior, la función de error podría quedar así:\n",
    " \n",
    " $\\mathscr{L}= -Y \\cdot log(\\hat{y}) -(1-Y) \\cdot log(1-\\hat{y})$\n",
    " \n",
    " por lo tanto la función de costo :\n",
    " \n",
    " $J=\\frac{1}{m}{‎‎\\sum}_{i=1}^m \\mathscr{L}^i $\n",
    " \n",
    " queda de la siguiente manera:\n",
    " \n",
    "  $J=-\\frac{1}{m}{‎‎\\sum}_{i=1}^m (y \\cdot log(\\hat{y}) -(1-y) \\cdot log(1-\\hat{y}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(.01, .9, 15)\n",
    "Y = -np.log(1-X)\n",
    "#print(X2)\n",
    "#print(X3)\n",
    "plt.plot(X,Y,\"bo\")\n",
    "plt.xlabel(\"$X$\")\n",
    "plt.ylabel(\"J\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def escalar(X):\n",
    "    xmin = np.min(X, axis=0)\n",
    "    xmax = np.max(X, axis=0)\n",
    "    xmean = np.mean(X, axis=0)\n",
    "    xrango = xmax - xmin\n",
    "    X = (X - xmean) / (xrango)\n",
    "    return (xmean, xrango, X)\n",
    "\n",
    "\n",
    "def sigmoide(t):\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "\n",
    "def getRL(m, n, X, Y):\n",
    "    b = 0\n",
    "    W = np.zeros([n-1, 1])\n",
    "    alpha = 0.1\n",
    "    #print(f\"Y= {Y} \")\n",
    "    for iter1 in range(50000**2):\n",
    "        yp = sigmoide((np.dot(X, W)+b))\n",
    "        #print(f\"yp= {yp} \")\n",
    "        acum0 = (Y * np.log(yp)) - (1-Y) * np.log(1-yp)\n",
    "        #print(f\"acum0= {acum0}\")\n",
    "        acum1 = acum0 * X\n",
    "        acum0 = np.mean(acum0, axis=0)\n",
    "        acum1 = np.mean(acum1, axis=0)\n",
    "        acum1 = acum1.reshape([n-1, 1])\n",
    "        b -= acum0 * alpha\n",
    "        W -= acum1 * alpha\n",
    "    return b, W\n",
    "\n",
    "\n",
    "def getPrediccionRL(xmean, xrango, b, W, X):\n",
    "    # escalamiento con los parametros xmena y xrango\n",
    "    X = (X - xmean) / (xrango)\n",
    "    Yp = sigmoide((np.dot(X, W)+b))\n",
    "    return Yp\n",
    "\n",
    "\n",
    "datos = pd.read_csv(\"./data/craneostibet.csv\")\n",
    "# craneostibet\n",
    "# fronteralinealdecision\n",
    "# Flor de Iris:\n",
    "# setosa\n",
    "# versicolor\n",
    "# virginica\n",
    "datos = datos.to_numpy()\n",
    "#print(f\"datos= \\n{datos}\")\n",
    "m, n = datos.shape\n",
    "X = datos[:, 0:n-1]\n",
    "Y = datos[:, n-1:n]\n",
    "\n",
    "\n",
    "X0 = [j for j in range(m) if Y[j] <= 0.5]\n",
    "X1 = [j for j in range(m) if Y[j] > 0.5]\n",
    "\n",
    "\n",
    "plt.plot(X[X0, 0], X[X0, 1], \"bo\")\n",
    "plt.plot(X[X1, 0], X[X1, 1], \"ro\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.grid()\n",
    "plt.title(\"Datos originales de entrenamiento\")\n",
    "plt.show()\n",
    "\n",
    "xmean, xrango, X2 = escalar(X)\n",
    "b, W = getRL(m, n, X2, Y)\n",
    "#X = datos[:,0:n-1]\n",
    "Yp = getPrediccionRL(xmean, xrango, b, W, X)\n",
    "#print(f\"b= {b}\")\n",
    "#print(f\"W= {W}\")\n",
    "#print(f\"Yp= \\n{Yp}\")\n",
    "\n",
    "for index, yp in enumerate(Yp):\n",
    "    if (yp <= 0.5):\n",
    "        Yp[index] = 0\n",
    "    else:\n",
    "        Yp[index] = 1\n",
    "\n",
    "datosF = []\n",
    "falsosP = []\n",
    "falsosN = []\n",
    "for index, d in enumerate(datos):\n",
    "    datosF.append([])\n",
    "    for value in d:\n",
    "        datosF[-1].append(value)\n",
    "    if (datosF[-1][-1] != Yp[index][0]):\n",
    "        if (datosF[-1][-1] == 1):\n",
    "            falsosP.append(index)\n",
    "        else:\n",
    "            falsosN.append(index)\n",
    "    datosF[-1].append(Yp[index][0])\n",
    "\n",
    "# 1 - 0 -> Falso N\n",
    "# 0 - 1 -> Falso P\n",
    "\n",
    "print(tabulate(datosF, headers=\"medida1,medida2,medida3,medida4,medida5,Tipo, Y'\".split(\",\")))\n",
    "\n",
    "print(f\"Total de falsos positivos: {len(falsosP)}. Indexes: {falsosP}. Porcentaje: {len(falsosP) / len(datosF)}\")\n",
    "print(f\"Total de falsos negativos: {len(falsosN)}. Indexes: {falsosN}. Porcentaje: {len(falsosN) / len(datosF)}\")\n",
    "\n",
    "X0 = [j for j in range(m) if Yp[j] <= 0.5]\n",
    "X1 = [j for j in range(m) if Yp[j] > 0.5]\n",
    "\n",
    "plt.plot(X[X0, 0], X[X0, 1], \"bo\")\n",
    "plt.plot(X[X1, 0], X[X1, 1], \"ro\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.grid()\n",
    "plt.title(\"Regresión Logística\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f0c57bc076e9217fce7fc7479a9e5a03ae5ebbc762fd5e8e7e34411aa91b3ee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
